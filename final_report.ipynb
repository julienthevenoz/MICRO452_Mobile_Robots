{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Basics of Mobile Robotics - 2024\n",
    "\n",
    "## Table of Contents\n",
    "0. [Introduction](#Introduction)\n",
    "1. [Setup](#Partie-1-:-Setup)\n",
    "    - 1.1 : Physical setup\n",
    "    - 1.2 : Code setup\n",
    "2. [Initial tuning](#Partie-2-:-Initial-tuning)\n",
    "    - 2.1 : Wheel speed (differential)\n",
    "3. [Vision](#Partie-2-:-Vision)\n",
    "    - 3.1 : Dynamic lighting adaptation\n",
    "    - 3.2 : Aruko markers\n",
    "        - 3.2.1 : Thymio and goal\n",
    "    - 3.2 : Map resizing\n",
    "4. [Kalmann](#Partie-3-:-Kalmann)\n",
    "    - 4.1 : Theory\n",
    "    - 4.2 : Implementation\n",
    "5. [Global path planning](#Partie-4-:-Global-path-planning)\n",
    "    - 5.1 : Dijkstra\n",
    "6. [Local navigation](#Partie-5-:-Local-navigation)\n",
    "    - 6.1 : Local obstacle detection\n",
    "    - 6.2 : Potential fields\n",
    "7. [Motion control](#Partie-5-:-Motion-control)\n",
    "    - 7.1 : Differential drive\n",
    "    - 7.2 : P-conntroler\n",
    "8. [Demonstrations](#Partie-5-:-emonstrations)\n",
    "9. [Conclusion](#IConclusion)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Authors : Tifaine Mezencev, Julien, Zhuoran, Christy\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 : Setup\n",
    "\n",
    "### 1.1 : Physical setup\n",
    "\n",
    "```Explication des données utilisées.```\n",
    "\n",
    "A small but determined vessel navigating uncharted waters, its sights set on a glittering treasure at the far edge of the map. Along the way, it must skillfully avoid perilous islands and evade the watchful patrols of navy ships.\n",
    "\n",
    "In this project, we implement global and local navigation for the Thymio robot. The goal is to enable the robot to plot an optimal course toward its destination while dynamically avoiding both static and unexpected obstacles. The key features of our implementation include:\n",
    "\n",
    "- Accurate map creation and feature localization using ArUco markers\n",
    "- Global pathfinding using Dijkstra's algorithm\n",
    "- Seamless connection to the Thymio robot for real-time path execution\n",
    "- Path-following with dynamic adjustment for precision navigation\n",
    "- Emergency handling, including \"Thymio kidnapping\" detection and recovery\n",
    "- Local obstacle avoidance with a potential field method\n",
    "- Robust navigation supported by a Kalman filter in the absence of camera input\n",
    "\n",
    "With this project, we aim to demonstrate how a small robot can overcome big challenges, transforming obstacles into opportunities and bringing our vision of autonomous navigation to life!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 : Code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Initial tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Wheel speed (differential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Vision\n",
    "\n",
    "The Vision.py module has two tasks : run the camera and analyse the frames captured by the camera. This is why the Vision class is actually more of a wrapper around the functionnalities given by the two other classes defined in the vision file, appropriately named Analysis and CameraFeed. \n",
    "\n",
    "\n",
    "Let's dive into Analysis.\n",
    " \n",
    "Given an image, the class has three tasks : pinpoint the thymio, the goal, and the 2D (black) obstacle in a consistent reference frame.  Of course doing this is easier said than done. A getter function of Vision will then be used to access these values and pass them on to the other modules.\n",
    "The first step is defining what is in the map and what is outside the map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Map\n",
    "\n",
    "We first wanted to use color to differenciate the components of our problem and we painted our map in blue and obstacles in black. Specifically, we tried using a blue color mask to make out the map and pinpoint its four corners. This was relavitely inconsistent however, and we decided to use ArUco markers to make our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_obstacle_corners(self, img):\n",
    "    \"\"\"\n",
    "    Detects obstacle edges in the image, approximates them as polygons, \n",
    "    and keeps only those with an average color that resembles black (obstacles).\n",
    "    Returns three outputs: the polygons, the binary obstacle mask, and the image with visualized polygons.\n",
    "    \"\"\"\n",
    "    # 1. Convert the image to LAB color space for color analysis\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # 2. Calculate the histogram of the L component\n",
    "    hist = cv2.calcHist([lab_img[:, :, 0]], [0], None, [180], [0, 180])  # 180 bins for L in the range [0, 180]\n",
    "    signal = np.gradient(medfilt(hist.flatten(), 15))  # Median filtering to smooth the signal\n",
    "\n",
    "    grad_threshold = -10\n",
    "    highest = 125\n",
    "\n",
    "    # 3. Search for the threshold in the signal\n",
    "    crossing_index = -1\n",
    "    for i in range(highest - 1, 1, -1):  # Start just below 'highest' and move backwards\n",
    "        if signal[i] < grad_threshold:\n",
    "            crossing_index = i\n",
    "            break\n",
    "\n",
    "    # # Find the threshold\n",
    "    # # if crossing_index != -1:\n",
    "    # #     print(f\"The highest index where the signal crosses {grad_threshold} is {crossing_index}.\")\n",
    "    # # else:\n",
    "    # #     print(f\"No crossing found before index {highest}.\")\n",
    "\n",
    "    treshold = crossing_index\n",
    "\n",
    "    # 4. Create a binary mask based on the threshold\n",
    "    mask = lab_img[:, :, 0] < treshold\n",
    "    obstacles = np.uint8(mask * 255)\n",
    "\n",
    "    # 5. Process connected components\n",
    "    COUNT = 3000  # Minimum size threshold for components\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(obstacles, connectivity=8)\n",
    "    filtered_mask = np.zeros_like(obstacles)\n",
    "\n",
    "    for i in range(1, num_labels):  # Ignore background (label 0)\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= COUNT:\n",
    "            filtered_mask[labels == i] = 255\n",
    "\n",
    "    # 6. Morphological operations to smooth the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Structuring element for dilation\n",
    "    filtered_mask = cv2.dilate(filtered_mask, kernel)\n",
    "    filtered_mask = cv2.erode(filtered_mask, kernel)\n",
    "\n",
    "    # 7. Find contours of the obstacles\n",
    "    contours, _ = cv2.findContours(filtered_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 8. List to store the corners of the obstacles\n",
    "    obstacle_corners = []\n",
    "\n",
    "    # 9. Approximate the contours as polygons\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)  # Approximation precision\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        # Add the corners of the polygons\n",
    "        obstacle_corners.append(approx.reshape(-1, 2).tolist())\n",
    "\n",
    "    # 10. Visualize the polygons with colored edges and circles at corners\n",
    "    img_with_polygons = img.copy()\n",
    "\n",
    "    # Assign a unique color for each polygon\n",
    "    for i, polygon in enumerate(obstacle_corners):\n",
    "        # Random color for each polygon (BGR format)\n",
    "        color = np.random.randint(0, 256, 3).tolist()\n",
    "\n",
    "        # Draw the corners as red circles\n",
    "        for (x, y) in polygon:\n",
    "            cv2.circle(img_with_polygons, (x, y), 5, (0, 0, 255), -1)  # Red circles at corners\n",
    "\n",
    "        # Draw the polygon edges\n",
    "        polygon_array = np.array(polygon, dtype=np.int32)\n",
    "        cv2.drawContours(img_with_polygons, [polygon_array], -1, color, 2)  # Polygon edges with random color\n",
    "\n",
    "    # 11. Return the results: obstacle corners, binary mask, and image with visualized polygons\n",
    "    return obstacle_corners, filtered_mask, img_with_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 : Dynamic lighting adaptation\n",
    "\n",
    "This function processes an image to detect obstacles, identify their edges, and approximate their shapes as polygons. It returns the polygons, a binary mask of the detected obstacles, and an image visualizing the polygons\n",
    "\n",
    "What the function does :\n",
    "- **Convert the image to LAB color space:** It uses the L (lightness) component to identify dark regions (potential obstacles).\n",
    "- **Threshold the lightness component:** It creates a binary mask to isolate dark regions.\n",
    "- **Filter small components:** Only retains regions that meet a size threshold.\n",
    "- **Smooth the mask:** Applies morphological operations to clean up the mask.\n",
    "- **Find contours and approximate them as polygons:** These represent the shapes of obstacles.\n",
    "- **Visualize the polygons:** Draws the approximated polygons and their corners on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_obstacle_corners(self, img):\n",
    "    \"\"\"\n",
    "    Detects obstacle edges in the image, approximates them as polygons, \n",
    "    and keeps only those with an average color that resembles black (obstacles).\n",
    "    Returns three outputs: the polygons, the binary obstacle mask, and the image with visualized polygons.\n",
    "    \"\"\"\n",
    "    # 1. Convert the image to LAB color space for color analysis\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # 2. Calculate the histogram of the L component\n",
    "    hist = cv2.calcHist([lab_img[:, :, 0]], [0], None, [180], [0, 180])  # 180 bins for L in the range [0, 180]\n",
    "    signal = np.gradient(medfilt(hist.flatten(), 15))  # Median filtering to smooth the signal\n",
    "\n",
    "    grad_threshold = -10\n",
    "    highest = 125\n",
    "\n",
    "    # 3. Search for the threshold in the signal\n",
    "    crossing_index = -1\n",
    "    for i in range(highest - 1, 1, -1):  # Start just below 'highest' and move backwards\n",
    "        if signal[i] < grad_threshold:\n",
    "            crossing_index = i\n",
    "            break\n",
    "\n",
    "    # # Find the threshold\n",
    "    # # if crossing_index != -1:\n",
    "    # #     print(f\"The highest index where the signal crosses {grad_threshold} is {crossing_index}.\")\n",
    "    # # else:\n",
    "    # #     print(f\"No crossing found before index {highest}.\")\n",
    "\n",
    "    treshold = crossing_index\n",
    "\n",
    "    # 4. Create a binary mask based on the threshold\n",
    "    mask = lab_img[:, :, 0] < treshold\n",
    "    obstacles = np.uint8(mask * 255)\n",
    "\n",
    "    # 5. Process connected components\n",
    "    COUNT = 3000  # Minimum size threshold for components\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(obstacles, connectivity=8)\n",
    "    filtered_mask = np.zeros_like(obstacles)\n",
    "\n",
    "    for i in range(1, num_labels):  # Ignore background (label 0)\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= COUNT:\n",
    "            filtered_mask[labels == i] = 255\n",
    "\n",
    "    # 6. Morphological operations to smooth the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Structuring element for dilation\n",
    "    filtered_mask = cv2.dilate(filtered_mask, kernel)\n",
    "    filtered_mask = cv2.erode(filtered_mask, kernel)\n",
    "\n",
    "    # 7. Find contours of the obstacles\n",
    "    contours, _ = cv2.findContours(filtered_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 8. List to store the corners of the obstacles\n",
    "    obstacle_corners = []\n",
    "\n",
    "    # 9. Approximate the contours as polygons\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)  # Approximation precision\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        # Add the corners of the polygons\n",
    "        obstacle_corners.append(approx.reshape(-1, 2).tolist())\n",
    "\n",
    "    # 10. Visualize the polygons with colored edges and circles at corners\n",
    "    img_with_polygons = img.copy()\n",
    "\n",
    "    # Assign a unique color for each polygon\n",
    "    for i, polygon in enumerate(obstacle_corners):\n",
    "        # Random color for each polygon (BGR format)\n",
    "        color = np.random.randint(0, 256, 3).tolist()\n",
    "\n",
    "        # Draw the corners as red circles\n",
    "        for (x, y) in polygon:\n",
    "            cv2.circle(img_with_polygons, (x, y), 5, (0, 0, 255), -1)  # Red circles at corners\n",
    "\n",
    "        # Draw the polygon edges\n",
    "        polygon_array = np.array(polygon, dtype=np.int32)\n",
    "        cv2.drawContours(img_with_polygons, [polygon_array], -1, color, 2)  # Polygon edges with random color\n",
    "\n",
    "    # 11. Return the results: obstacle corners, binary mask, and image with visualized polygons\n",
    "    return obstacle_corners, filtered_mask, img_with_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : ArUco markers\n",
    "ArUco markers are binary patterns which are unique and have no symmetry. Due to this, they are often used in robotics and computer vision because seeing one markers makes it possible to completely determine the orientation of the camera relative to the marker (and its distance, if the size of the marker is known). They are part of a very convenient library which implements most of the code needed to detect the markers and the according translation and rotation transform matrices.\n",
    "\n",
    "We have 6 markers : one per map corners, and two for the goal and thymio.\n",
    "\n",
    "We wrote a function to find the center point and 2D orientation (defineed as angle between marker top edge and image X-axis). \n",
    "Yes, the aruco library already implements a rotation transform function, yes we forgot to read the docs before starting. But it's okay, it was a fun little geometry puzzle to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : Map resizing\n",
    "It is easier for us if the image is a perpendicular top-down view and everything that isn't part of the map is cropped out. To achieve this, we apply some visual transformations on the image. First we find the four corners of the map with our aruco markers We are lucky because opencv provides us with great tools to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 : Map resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Camera management \n",
    "We want the camera to always be running in the background and display the video feed continously (along with whichever debugging displays), independently of what the path planner, control or kalman modules are doing. \n",
    "\n",
    "This is why we use the ```threading``` library, which allows different processes to be run in parallel. The CameraFeed class inherits from the ```threading```.Thread class. \n",
    "Its core method is a while loop that captures a frame, finds its corners, resizes it, detects the Thyimio & goal and (if desired) draws and shows the debugging displays. \n",
    "\n",
    "At the start of the code, the begin() method of the Vision class initializes and runs a CameraFeed thread object, which will continue its work in the background for the rest of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 : Kalmann filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, there are situations where the camera may be obstructed or go offline. In such cases, the camera measurements become less reliable, so we switch to using the motor sensors to estimate the robot's state. While the motor sensors may not provide the same level of accuracy, they serve as a reliable backup option. To integrate all sensor data effectively, we use a Kalman filter, which allows us to combine the information from both sensors to provide a more accurate and robust estimation of the robot’s state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 : Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real model, there is noise present both in the state transition model and in the measurement model, as shown in the formulation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_k = F_kx_{k-1} + B_k u_k + w_k\n",
    "$$\n",
    "$$\n",
    "z_k = H_kx_k + v_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Thymio robot, the $\\omega _k$ represents the mechanical error of the wheel motors, while \n",
    "$v _k$ refers to the measurement error caused by the camera we use. To implement the Kalman filter, we follow two main steps: the prediction step and the update step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prediction step, we predicts the state of the system and the associated uncertainty based on the previous state estimate.\n",
    "$$\n",
    "\\hat{x}_{k|k-1} = F_k\\hat{x}_{k-1|k-1} + B_k u_k\n",
    "$$\n",
    "$$\n",
    "P_{k|k-1} = F_kP_{k-1|k-1}{F_k}^T + Q_k\n",
    "$$\n",
    "In the update step, we incorporate the actual measurement into the prediction to update the state estimte.\n",
    "$$\n",
    "\\tilde{y}_k = z_k - H_k\\hat{x}_{k|k-1}\n",
    "$$\n",
    "$$\n",
    "S_k = H_kP_{k|k-1}{H_k}^T + R_k\n",
    "$$\n",
    "$$\n",
    "K_k = P_{k|k-1}{H_k}^T{S_k}^{-1}\n",
    "$$\n",
    "$$\n",
    "\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k\\tilde{y}_k\n",
    "$$\n",
    "$$\n",
    "P_{k|k} = (I - K_kH_k)P_{k|k-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our system is nonlinear, we need to linearize it before applying the extended Kalman filter. We update our robot's state at a very high frequency, which allows us to use a simple linearization method. During such small time intervals, we assume that the Thymio moves in a straight line, thus we get our state transition matrix as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}x_{k}\\\\ y_{k} \\\\ \\theta_{k}\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\\begin{bmatrix}x_{k-1} \\\\ y_{k-1} \\\\ {\\theta}_{k-1}\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "cos\\theta / 2 & cos\\theta / 2  \\\\\n",
    "sin\\theta / 2 & sin\\theta / 2 \\\\\n",
    "1 / 2L & -1 / 2L\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\Delta s_l \\\\ \\Delta s_r \\end{bmatrix} + \\omega _k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the measurement model, we directly get the robot's state from the camera, so we could simply set $H$ to be a identity matrix, as show below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z_k = H_kx_k +v_k\n",
    "$$\n",
    "in which:\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole implementation function is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(self, vl, vr, measurement):\n",
    "    measurement = np.array(measurement)\n",
    "    u = np.array([vl, vr])\n",
    "    A = self.A\n",
    "    print(\"change something\")\n",
    "    B = np.array([[np.cos(self.kal_state[2]) / 2, np.cos(self.kal_state[2]) / 2],\n",
    "                      [np.sin(self.kal_state[2]) / 2, np.sin(self.kal_state[2]) / 2],\n",
    "                      [1 / (2 * self.L), - 1 / (2 * self.L) ]])\n",
    "    est_state = A @ (self.kal_state) + B @ u\n",
    "    est_variance = A @ (self.kal_variance @ A.T) + self.Q\n",
    "    yk = measurement - self.H @ est_state\n",
    "    self.S = self.H @ (est_variance @ self.H.T) + self.R\n",
    "    self.K = est_variance @ (self.H.T @ np.linalg.inv(self.S))\n",
    "    self.kal_state = est_state + self.K @ yk\n",
    "    self.kal_variance = (np.eye(3) - self.K @ self.H) @ est_variance\n",
    "    return self.kal_state, self.kal_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 : Global path planning\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 : Dijkstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 : Local navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 : Local obstacle detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_obstacle(self, prox_horizontal):\n",
    "    \n",
    "    mark = 0\n",
    "    for i in range(5):\n",
    "      if prox_horizontal[i] > self.threshold_high:\n",
    "        return 1\n",
    "      if prox_horizontal[i] < self.threshold_low:\n",
    "        mark = mark + 1\n",
    "    if mark == 5:\n",
    "      return 0\n",
    "    else:\n",
    "      return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 : Potential fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function enables local navigation by adjusting the robot's motor speeds to avoid obstacles based on proximity sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance(self):\n",
    "    speed = self.get_motor_speed()\n",
    "    prox_horizontal = self.read_prox_sensors()\n",
    "    while self.is_obstacle(prox_horizontal):\n",
    "      delta = 0\n",
    "      for i in range(5):\n",
    "        delta += prox_horizontal[i] * self.obstSpeedGain[i]\n",
    "      delta = max(-self.max_omega, min(delta, self.max_omega))\n",
    "      speed[0] = speed[0] + delta\n",
    "      speed[1] = speed[1] - delta\n",
    "      speed[0] = int(speed[0])\n",
    "      speed[1] = int(speed[1])\n",
    "      self.thymio.set_motor_speed(speed[0], speed[1])\n",
    "      speed = self.get_motor_speed()\n",
    "      prox_horizontal = self.read_prox_sensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 : Motion control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 : Differential drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 : Astolfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 : Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 : Conclusion\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
